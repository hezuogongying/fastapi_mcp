import streamlit as st
import httpx
import json
from openai import OpenAI
import os
import logging

# --- é…ç½® ---
# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(page_title="MCP-LLM äº¤äº’æ¼”ç¤º", page_icon="ğŸ¤–")

# ä» Streamlit secrets æˆ–ç¯å¢ƒå˜é‡è·å– OpenAI é…ç½®
# åœ¨æœ¬åœ°è¿è¡Œæ—¶ï¼Œå¯ä»¥è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
# $env:OPENAI_API_KEY="your_key"
# $env:OPENAI_API_BASE="http://127.0.0.1:8080/v1" (å¦‚æœä½ ä½¿ç”¨æœ¬åœ°LLM)
api_key = st.secrets.get("OPENAI_API_KEY", os.environ.get("OPENAI_API_KEY", "none"))
base_url = st.secrets.get("OPENAI_API_BASE", os.environ.get("OPENAI_API_BASE"))

# FastAPI æœåŠ¡å™¨çš„åœ°å€
FASTAPI_BASE_URL = "http://127.0.0.1:8000"
OPENAPI_URL = f"{FASTAPI_BASE_URL}/openapi.json"

# --- æ ¸å¿ƒå‡½æ•° ---

@st.cache_data(ttl=300) # ç¼“å­˜ OpenAPI schema 5åˆ†é’Ÿ
def get_api_tools_from_schema():
    """ä» FastAPI æœåŠ¡å™¨è·å–å¹¶æ ¼å¼åŒ– MCP å·¥å…·åˆ—è¡¨"""
    try:
        logging.info(f"æ­£åœ¨ä» {OPENAPI_URL} è·å– OpenAPI schema...")
        response = httpx.get(OPENAPI_URL)
        response.raise_for_status()
        schema = response.json()
        logging.info("æˆåŠŸè·å–å¹¶è§£æ OpenAPI schemaã€‚")
        
        tools = []
        tool_mappings = {}

        for path, path_item in schema.get("paths", {}).items():
            for method, operation in path_item.items():
                if "tags" in operation and "mcp-tools" in operation["tags"]:
                    # ç¡®ä¿å‡½æ•°åå¯¹äºLLMæ˜¯æœ‰æ•ˆçš„
                    function_name = operation.get("operationId").replace("-", "_")
                    description = operation.get("description") or operation.get("summary", "")
                    
                    parameters = {"type": "object", "properties": {}, "required": []}
                    
                    # å¤„ç†URLè·¯å¾„å‚æ•°å’ŒæŸ¥è¯¢å‚æ•°
                    if "parameters" in operation:
                        for param in operation["parameters"]:
                            param_name = param["name"]
                            param_schema = param.get("schema", {})
                            parameters["properties"][param_name] = {
                                "type": param_schema.get("type"),
                                "description": param.get("description", "")
                            }
                            if param.get("required", False):
                                parameters["required"].append(param_name)
                    
                    # å¤„ç†POST/PUTè¯·æ±‚çš„è¯·æ±‚ä½“
                    if "requestBody" in operation:
                        content = operation["requestBody"].get("content", {})
                        if "application/json" in content:
                            request_schema = content["application/json"].get("schema", {})
                            # å°†è¯·æ±‚ä½“ä½œä¸ºä¸€ä¸ªåä¸º 'data' çš„å‚æ•°
                            parameters["properties"]["data"] = request_schema
                            if operation["requestBody"].get("required", False):
                                parameters["required"].append("data")

                    tools.append({
                        "type": "function",
                        "function": {
                            "name": function_name,
                            "description": description,
                            "parameters": parameters
                        }
                    })
                    tool_mappings[function_name] = {
                        "method": method.upper(),
                        "path": path
                    }
        logging.info(f"æˆåŠŸè½¬æ¢ {len(tools)} ä¸ªå·¥å…·ã€‚")
        return tools, tool_mappings
    except httpx.RequestError as e:
        st.error(f"æ— æ³•è¿æ¥åˆ° FastAPI æœåŠ¡å™¨: {e}ã€‚è¯·ç¡®ä¿ [02_full_schema_description_example.py](cci:7://file:///d:/project/python/fastapi_mcp/examples-zh/02_full_schema_description_example.py:0:0-0:0) æ­£åœ¨è¿è¡Œã€‚")
        return None, None
    except Exception as e:
        st.error(f"è§£æ OpenAPI schema å¤±è´¥: {e}")
        logging.error(f"è§£æ OpenAPI schema å¤±è´¥: {e}", exc_info=True)
        return None, None

def execute_api_tool(tool_call, tool_mappings):
    """æ‰§è¡Œ LLM è¯·æ±‚çš„ API å·¥å…·è°ƒç”¨"""
    function_name = tool_call.function.name
    try:
        arguments = json.loads(tool_call.function.arguments)
    except json.JSONDecodeError:
        return f"é”™è¯¯: æ— æ•ˆçš„å‚æ•°æ ¼å¼: {tool_call.function.arguments}"
        
    mapping = tool_mappings.get(function_name)
    if not mapping:
        return f"é”™è¯¯: æœªçŸ¥çš„å·¥å…· '{function_name}'"
        
    method = mapping["method"]
    path_template = mapping["path"]
    
    # åˆ†ç¦»è·¯å¾„å‚æ•°å’ŒæŸ¥è¯¢/è¯·æ±‚ä½“å‚æ•°
    path_params = {}
    other_params = {}
    for key, value in arguments.items():
        if f"{{{key}}}" in path_template:
            path_params[key] = value
        else:
            other_params[key] = value
            
    url = f"{FASTAPI_BASE_URL}{path_template.format(**path_params)}"
    
    try:
        logging.info(f"æ­£åœ¨æ‰§è¡Œå·¥å…·: {method} {url} with params {other_params}")
        with httpx.Client() as client:
            response = None
            if method == "GET":
                response = client.get(url, params=other_params)
            elif method == "POST":
                response = client.post(url, json=other_params.get("data"))
            elif method == "PUT":
                 response = client.put(url, json=other_params.get("data"))
            elif method == "DELETE":
                response = client.delete(url)
            else:
                return f"é”™è¯¯: ä¸æ”¯æŒçš„ HTTP æ–¹æ³• '{method}'"

            response.raise_for_status()
            return response.json()
    except httpx.HTTPStatusError as e:
        logging.error(f"API è°ƒç”¨å¤±è´¥: {e.response.status_code} - {e.response.text}")
        return f"API è°ƒç”¨å¤±è´¥: {e.response.status_code} - {e.response.text}"
    except Exception as e:
        logging.error(f"æ‰§è¡Œ API è°ƒç”¨æ—¶å‡ºé”™: {e}", exc_info=True)
        return f"æ‰§è¡Œ API è°ƒç”¨æ—¶å‡ºé”™: {e}"

# --- Streamlit UI ---

st.title("ğŸ¤– MCP-LLM äº¤äº’æ¼”ç¤º")
st.caption("ä¸€ä¸ªä½¿ç”¨ LLM ä¸ FastApiMCP å·¥å…·äº¤äº’çš„ Streamlit èŠå¤©æœºå™¨äºº")

# åˆå§‹åŒ–
if "client" not in st.session_state:
    if not api_key or api_key == "none":
        st.warning("è¯·åœ¨ç¯å¢ƒå˜é‡æˆ– Streamlit secrets ä¸­è®¾ç½® `OPENAI_API_KEY`ã€‚")
        st.stop()
    st.session_state.client = OpenAI(api_key=api_key, base_url=base_url)

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘å¯ä»¥å¸®ä½ æŸ¥è¯¢ã€æ·»åŠ ã€ä¿®æ”¹æˆ–åˆ é™¤ç‰©å“ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"}]

# è·å–å·¥å…·
tools, tool_mappings = get_api_tools_from_schema()
if not tools:
    st.stop()

# æ˜¾ç¤ºèŠå¤©è®°å½•
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# èŠå¤©è¾“å…¥
if prompt := st.chat_input("ä¾‹å¦‚: 'å¸®æˆ‘æ‰¾æ‰¾ä»·æ ¼ä½äº10å…ƒçš„é”¤å­'"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # è°ƒç”¨ LLM
    try:
        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                response = st.session_state.client.chat.completions.create(
                    model="gpt-4-turbo", # æˆ–æ‚¨ä½¿ç”¨çš„æ¨¡å‹
                    messages=[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
                    tools=tools,
                    tool_choice="auto",
                )
                response_message = response.choices[0].message
                tool_calls = response_message.tool_calls

            if tool_calls:
                st.session_state.messages.append(response_message)
                for tool_call in tool_calls:
                    st.info(f"æ­£åœ¨è°ƒç”¨å·¥å…·: `{tool_call.function.name}`...")
                    function_response = execute_api_tool(tool_call, tool_mappings)
                    st.session_state.messages.append({
                        "tool_call_id": tool_call.id,
                        "role": "tool",
                        "name": tool_call.function.name,
                        "content": json.dumps(function_response, ensure_ascii=False),
                    })
                
                with st.spinner("å¤„ç†å·¥å…·ç»“æœ..."):
                    second_response = st.session_state.client.chat.completions.create(
                        model="gpt-4-turbo",
                        messages=[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
                    )
                    final_response = second_response.choices[0].message.content
                    st.markdown(final_response)
                    st.session_state.messages.append({"role": "assistant", "content": final_response})
            else:
                final_response = response_message.content
                st.markdown(final_response)
                st.session_state.messages.append({"role": "assistant", "content": final_response})

    except Exception as e:
        st.error(f"ä¸ LLM é€šä¿¡æ—¶å‡ºé”™: {e}")
        logging.error(f"LLM è°ƒç”¨å‡ºé”™: {e}", exc_info=True)